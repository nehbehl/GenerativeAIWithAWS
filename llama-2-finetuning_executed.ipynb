{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Fine-tune LLaMA 2 models on SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0679163a-387a-4ba6-8ce0-d5571614c0dc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|llama-2-text-completion.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251624f9-1eb6-4051-a774-0a4ba83cabf5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy pre-trained Llama 2 model as well as fine-tune it for your dataset in domain adaptation or instruction tuning format.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9b99d-639b-40f3-91e3-1fe00ee032a4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Model License information\n",
    "---\n",
    "To perform inference on these models, you need to pass custom_attributes='accept_eula=true' as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets custom_attributes='accept_eula=false', so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by '=' and pairs are separated by ';'. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if 'accept_eula=false; accept_eula=true' is passed to the server, then 'accept_eula=true' is kept and passed to the script handler.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c4fcd-d6c5-4381-8425-1d224c0ac197",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Set up\n",
    "\n",
    "---\n",
    "We begin by installing and upgrading necessary packages. Restart the kernel after executing the cell below for the first time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85addd9d-ec89-44a7-9fb5-9bc24fe9993b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.203.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.203.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.34.13)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.26.1)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.24.4)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.19.1)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.11.0)\n",
      "Requirement already satisfied: tblib<3,>=1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: urllib3<1.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.26.18)\n",
      "Requirement already satisfied: uvicorn==0.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.22.0)\n",
      "Requirement already satisfied: fastapi==0.95.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.95.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.66.1)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (5.9.5)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker) (1.10.13)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker) (0.27.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.12.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.34.13)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->sagemaker) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->sagemaker) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->sagemaker) (2023.7.22)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from docker->sagemaker) (1.6.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3.post1)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (4.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (1.1.3)\n",
      "Downloading sagemaker-2.203.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, multidict, frozenlist, async-timeout, yarl, huggingface-hub, aiosignal, aiohttp, sagemaker, datasets\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.203.0\n",
      "    Uninstalling sagemaker-2.203.0:\n",
      "      Successfully uninstalled sagemaker-2.203.0\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.16.1 frozenlist-1.4.1 huggingface-hub-0.20.2 multidict-6.0.4 pyarrow-hotfix-0.6 sagemaker-2.203.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {},
   "source": [
    "## Deploy Pre-trained Model\n",
    "\n",
    "---\n",
    "\n",
    "First we will deploy the Llama-2 model as a SageMaker endpoint. To train/deploy 13B and 70B models, please change model_id to \"meta-textgeneration-llama-2-7b\" and \"meta-textgeneration-llama-2-70b\" respectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e01401-82db-4d49-9457-f930f4138618",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"2.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For forward compatibility, pin to model_version='2.*' in your JumpStartModel or JumpStartEstimator definitions. Note that major version upgrades may have different EULA acceptance terms and input/output signatures.\n",
      "Using model 'meta-textgeneration-llama-2-7b' with wildcard version identifier '2.*'. You can pin to version '2.1.8' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "pretrained_predictor = pretrained_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017c4ef-eb89-4da6-8e28-c800adbfc4b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "---\n",
    "Next, we invoke the endpoint with some sample queries. Later, in this notebook, we will fine-tune this model with a custom dataset and carry out inference using the fine-tuned model. We will also show comparison between results obtained via the pre-trained and the fine-tuned models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b795a085-048f-42b2-945f-0cd339c1cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd833f8-1ddc-4805-80b2-19e7db629880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      ">  to be happy and to help others be happy.\n",
      "I’m an optimist. I think the world is a beautiful place and I think we should all try to make it better.\n",
      "I believe that the meaning of life is to be happy and to help others be happy.\n",
      "I believe that the meaning of\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = pretrained_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6773e6-7cf2-4cea-bce6-905d5995d857",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "To learn about additional use cases of pre-trained model, please checkout the notebook [Text completion: Run Llama 2 models in SageMaker JumpStart](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/llama-2-text-completion.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19e16f-d459-40c6-9d6b-0272938b3878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. Please find more details in the section [Dataset instruction](#Dataset-instruction). In this demo, we will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. We will select the summarization examples for fine-tuning.\n",
    "\n",
    "\n",
    "Training data is formatted in JSON lines (.jsonl) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple jsonl files. The training folder can also contain a template.json file describing the input and output formats.\n",
    "\n",
    "To train your model on a collection of unstructured dataset (text files), please see the section [Example fine-tuning with Domain-Adaptation dataset format](#Example-fine-tuning-with-Domain-Adaptation-dataset-format) in the Appendix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580c091ebbbe4efdbc60657d6ac3e0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f13d5ab3a042b5a3f022d418c06b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c674ded5d1430fae24e4add70cf0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa0bcebe0b04f319f700e2a57c2a012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b30890043c4656b3c1a1f217e69038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2050031"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fbf002-3ee3-4cc8-8fce-871939f1bd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What type of cheeses can you use to make a grilled cheese sandwich.',\n",
       " 'context': 'A grilled cheese sandwich is made by placing a cheese filling, often cheddar or American cheese, between two slices of bread, which is then heated until the bread browns and the cheese melts. A layer of butter or mayonnaise may be added to the outside of the bread for additional flavor and texture. Alternatives may include additional ingredients, such as meat, peppers, tomatoes, or onions. Methods for heating the sandwich include cooking on a griddle, fried in a pan, or using a panini grill or sandwich toaster, the latter method more common in the United Kingdom, where the sandwiches are normally called \"toasted sandwiches\" or \"toasties\", in Australia, where they are called \"jaffles\" or \"toasted sandwiches\", and South Africa, where they are called “snackwiches”. Other methods include baking in an oven or toaster oven — or in a toasting bag in an electric toaster.',\n",
       " 'response': 'Common cheeses to make a grilled cheese are cheddar or American cheese, although you are not limited to those cheeses.  Depending on a persons preferences there are a large variety of cheeses that are well suited for a grilled cheese sandwich. Many people enjoy Swiss cheese, provolone, mozzarella cheese, brie, Monterey jack cheese, pepper jack cheese,  gouda, havarti, fontina and many others.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we create a prompt template for using the data in an instruction / input format for the training job (since we are instruction fine-tuning the model in this example), and also for inferencing the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e1ee29a-8439-4788-8088-35a433fe2110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://sagemaker-us-east-1-822679942835/dolly_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "---\n",
    "Next, we fine-tune the LLaMA v2 7B model on the summarization dataset from Dolly. Finetuning scripts are based on scripts provided by [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). To learn more about the fine-tuning scripts, please checkout section [5. Few notes about the fine-tuning method](#5.-Few-notes-about-the-fine-tuning-method). For a list of supported hyper-parameters and their default values, please see section [3. Supported Hyper-parameters for fine-tuning](#3.-Supported-Hyper-parameters-for-fine-tuning).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-7b-2024-01-13-12-27-51-506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-13 12:27:51 Starting - Starting the training job...\n",
      "2024-01-13 12:28:09 Starting - Preparing the instances for training....................................\n",
      "2024-01-13 12:34:23 Downloading - Downloading input data..............................\n",
      "2024-01-13 12:39:20 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-13 12:39:21,565 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-13 12:39:21,591 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-13 12:39:21,600 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-13 12:39:21,602 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-13 12:39:29,624 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+6e4932cda8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0.dev20231104+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+6e4932cda8->-r requirements.txt (line 17)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=06d2f245fb0616f26cd4e8c0ff62aa170f61198203327174cccb35b33139a412\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.10.1\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.10.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.10.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0.dev20231104+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+6e4932cda8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.2.0.dev20231104+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-01-13 12:40:53,913 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-13 12:40:53,913 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-13 12:40:53,963 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-13 12:40:53,996 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-13 12:40:54,030 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-13 12:40:54,039 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2024-01-13-12-27-51-506\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2024-01-13-12-27-51-506\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-01-13 12:40:54,084 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '1', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '1', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 759.29it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1069 examples [00:00, 106943.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 15997.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 390.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 392.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 391.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1789.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1782.47 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:40<00:00, 18.63s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:40<00:00, 20.31s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 438\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 110\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/109 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda11.8\u001b[0m\n",
      "\u001b[34malgo-1:64:103 [0] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:64:103 [0] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.6873152256011963\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/109 [00:05<09:42,  5.40s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.8070933818817139\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 2/109 [00:09<08:40,  4.86s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.6232116222381592\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 3/109 [00:14<08:16,  4.69s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.417858362197876\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 4/109 [00:18<08:03,  4.61s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.4891756772994995\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 5/109 [00:23<07:54,  4.56s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.478968620300293\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 6/109 [00:27<07:47,  4.54s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.609932780265808\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 7/109 [00:32<07:40,  4.52s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.7336753606796265\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 8/109 [00:36<07:35,  4.51s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.4650009870529175\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 9/109 [00:41<07:29,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.486954689025879\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 10/109 [00:45<07:24,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.4791978597640991\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 11/109 [00:50<07:20,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.3224984407424927\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 12/109 [00:54<07:15,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.7792433500289917\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 13/109 [00:59<07:10,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.4531677961349487\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 14/109 [01:03<07:07,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.4231523275375366\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 15/109 [01:08<07:02,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.3808412551879883\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 16/109 [01:12<06:57,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.3881256580352783\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 17/109 [01:17<06:52,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.2925803661346436\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 18/109 [01:21<06:48,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.6275291442871094\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 19/109 [01:26<06:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.0560251474380493\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 20/109 [01:30<06:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.2051112651824951\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 21/109 [01:35<06:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.2699874639511108\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 22/109 [01:39<06:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.1595406532287598\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 23/109 [01:44<06:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.4265087842941284\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 24/109 [01:48<06:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.3690872192382812\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 25/109 [01:53<06:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.1839592456817627\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 26/109 [01:57<06:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.3467715978622437\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 27/109 [02:01<06:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.2320476770401\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 28/109 [02:06<06:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.3518803119659424\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 29/109 [02:10<05:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.4039583206176758\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 30/109 [02:15<05:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 1.3734315633773804\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 31/109 [02:19<05:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.3273868560791016\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 32/109 [02:24<05:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 1.5312894582748413\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 33/109 [02:28<05:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.299159049987793\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 34/109 [02:33<05:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 1.0473495721817017\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 35/109 [02:37<05:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.3665351867675781\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 36/109 [02:42<05:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.2717812061309814\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 37/109 [02:46<05:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 1.1442177295684814\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 38/109 [02:51<05:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 1.4846248626708984\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 39/109 [02:55<05:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.2139633893966675\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 40/109 [03:00<05:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.3693842887878418\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 41/109 [03:04<05:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 1.1374796628952026\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 42/109 [03:09<05:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 1.1181341409683228\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 43/109 [03:13<04:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.2765880823135376\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 44/109 [03:18<04:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.3502600193023682\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 45/109 [03:22<04:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.303985834121704\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 46/109 [03:27<04:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.1708757877349854\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 47/109 [03:31<04:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.5697757005691528\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 48/109 [03:36<04:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.606685757637024\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 49/109 [03:40<04:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.4006876945495605\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 50/109 [03:45<04:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.4901964664459229\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 51/109 [03:49<04:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.2651221752166748\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 52/109 [03:54<04:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.3297396898269653\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▊     #033[0m| 53/109 [03:58<04:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 1.1485346555709839\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m████▉     #033[0m| 54/109 [04:02<04:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 1.0784050226211548\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 55/109 [04:07<04:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 1.4174150228500366\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████▏    #033[0m| 56/109 [04:11<03:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.2321627140045166\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 57/109 [04:16<03:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.6250803470611572\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 58/109 [04:20<03:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 1.1446932554244995\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 59/109 [04:25<03:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.288983941078186\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▌    #033[0m| 60/109 [04:29<03:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 1.313276767730713\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 61/109 [04:34<03:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 1.0856456756591797\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 62/109 [04:38<03:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 1.420458436012268\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 63/109 [04:43<03:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 1.3728817701339722\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▊    #033[0m| 64/109 [04:47<03:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 1.2535520792007446\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m█████▉    #033[0m| 65/109 [04:52<03:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.4430385828018188\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 66/109 [04:56<03:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.2932008504867554\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████▏   #033[0m| 67/109 [05:01<03:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 1.099122405052185\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 68/109 [05:05<03:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.4055994749069214\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 69/109 [05:10<02:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 1.0318243503570557\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 70/109 [05:14<02:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.407606840133667\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 71/109 [05:19<02:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.315606713294983\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 72/109 [05:23<02:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.6492091417312622\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 73/109 [05:28<02:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.470980167388916\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 74/109 [05:32<02:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 1.3339612483978271\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 75/109 [05:37<02:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 1.1002309322357178\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m██████▉   #033[0m| 76/109 [05:41<02:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 1.5393621921539307\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 77/109 [05:46<02:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 1.2195665836334229\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 78/109 [05:50<02:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 1.1846715211868286\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 79/109 [05:54<02:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.2272100448608398\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 80/109 [05:59<02:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.2007389068603516\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 81/109 [06:03<02:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 1.5400173664093018\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 82/109 [06:08<02:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 1.2104147672653198\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 83/109 [06:12<01:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 1.2280076742172241\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 84/109 [06:17<01:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.2773637771606445\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 85/109 [06:21<01:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.2527661323547363\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 86/109 [06:26<01:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.285085916519165\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m███████▉  #033[0m| 87/109 [06:30<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 1.1300159692764282\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 88/109 [06:35<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.4285742044448853\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 89/109 [06:39<01:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 1.078668475151062\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 90/109 [06:44<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 1.0337496995925903\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 91/109 [06:48<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 1.2822279930114746\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 92/109 [06:53<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 1.0049422979354858\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 93/109 [06:57<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 1.5150119066238403\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 94/109 [07:02<01:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 1.1682989597320557\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 95/109 [07:06<01:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 1.17353093624115\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 96/109 [07:11<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 1.3999624252319336\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 97/109 [07:15<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 1.4131901264190674\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m████████▉ #033[0m| 98/109 [07:20<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 1.392400860786438\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 99/109 [07:24<00:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 1.161150574684143\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 100/109 [07:29<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 1.3595538139343262\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 101/109 [07:33<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 1.3073663711547852\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▎#033[0m| 102/109 [07:38<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 1.2968517541885376\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 103/109 [07:42<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 1.265242099761963\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▌#033[0m| 104/109 [07:47<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 1.2660725116729736\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 105/109 [07:51<00:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 0.9928516745567322\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 106/109 [07:55<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 1.072110652923584\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 107/109 [08:00<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 1.389567494392395\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 108/109 [08:04<00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 1.1875040531158447\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 109/109 [08:09<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 109/109 [08:09<00:00,  4.49s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 17 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 18 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 17 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/110 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 1/110 [00:00<00:47,  2.31it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 2/110 [00:00<00:46,  2.32it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 3/110 [00:01<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 4/110 [00:01<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 5/110 [00:02<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 6/110 [00:02<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▋         #033[0m| 7/110 [00:03<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 8/110 [00:03<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 9/110 [00:03<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 10/110 [00:04<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 11/110 [00:04<00:42,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 12/110 [00:05<00:41,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 13/110 [00:05<00:41,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 14/110 [00:06<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▎        #033[0m| 15/110 [00:06<00:40,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 16/110 [00:06<00:40,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▌        #033[0m| 17/110 [00:07<00:39,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▋        #033[0m| 18/110 [00:07<00:39,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 19/110 [00:08<00:38,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 20/110 [00:08<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 21/110 [00:09<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 22/110 [00:09<00:37,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 23/110 [00:09<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 24/110 [00:10<00:36,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 25/110 [00:10<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▎       #033[0m| 26/110 [00:11<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▍       #033[0m| 27/110 [00:11<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 28/110 [00:12<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▋       #033[0m| 29/110 [00:12<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 30/110 [00:12<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 31/110 [00:13<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 32/110 [00:13<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 33/110 [00:14<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 34/110 [00:14<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 35/110 [00:15<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 36/110 [00:15<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▎      #033[0m| 37/110 [00:15<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 38/110 [00:16<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 39/110 [00:16<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▋      #033[0m| 40/110 [00:17<00:29,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 41/110 [00:17<00:29,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 42/110 [00:18<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 43/110 [00:18<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 44/110 [00:18<00:28,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 45/110 [00:19<00:27,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 46/110 [00:19<00:27,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 47/110 [00:20<00:26,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▎     #033[0m| 48/110 [00:20<00:26,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 49/110 [00:20<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 50/110 [00:21<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 51/110 [00:21<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 52/110 [00:22<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 53/110 [00:22<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 54/110 [00:23<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 55/110 [00:23<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 56/110 [00:23<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 57/110 [00:24<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 58/110 [00:24<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 59/110 [00:25<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 60/110 [00:25<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 61/110 [00:26<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▋    #033[0m| 62/110 [00:26<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 63/110 [00:26<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 64/110 [00:27<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 65/110 [00:27<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 66/110 [00:28<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 67/110 [00:28<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 68/110 [00:29<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 69/110 [00:29<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▎   #033[0m| 70/110 [00:29<00:17,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 71/110 [00:30<00:16,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 72/110 [00:30<00:16,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▋   #033[0m| 73/110 [00:31<00:15,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 74/110 [00:31<00:15,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 75/110 [00:32<00:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 76/110 [00:32<00:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m███████   #033[0m| 77/110 [00:32<00:14,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 78/110 [00:33<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 79/110 [00:33<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 80/110 [00:34<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 81/110 [00:34<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▍  #033[0m| 82/110 [00:35<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 83/110 [00:35<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▋  #033[0m| 84/110 [00:35<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 85/110 [00:36<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 86/110 [00:36<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 87/110 [00:37<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 88/110 [00:37<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 89/110 [00:38<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 90/110 [00:38<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 91/110 [00:38<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▎ #033[0m| 92/110 [00:39<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▍ #033[0m| 93/110 [00:39<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 94/110 [00:40<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▋ #033[0m| 95/110 [00:40<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 96/110 [00:41<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 97/110 [00:41<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 98/110 [00:41<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 99/110 [00:42<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 100/110 [00:42<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 101/110 [00:43<00:03,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 102/110 [00:43<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▎#033[0m| 103/110 [00:44<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 104/110 [00:44<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 105/110 [00:44<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 106/110 [00:45<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 107/110 [00:45<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 108/110 [00:46<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 109/110 [00:46<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 110/110 [00:47<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 110/110 [00:47<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.4696, device='cuda:0') eval_epoch_loss=tensor(1.2440, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.244031310081482\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=3.7757, train_epoch_loss=1.3286, epcoh time 489.708312051s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/109 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.3360041379928589\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 1/109 [00:04<08:04,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.5723369121551514\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 2/109 [00:08<07:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.3923510313034058\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   3%|#033[34m▎         #033[0m| 3/109 [00:13<07:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.1732220649719238\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 4/109 [00:17<07:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.1991288661956787\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▍         #033[0m| 5/109 [00:22<07:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.304781198501587\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▌         #033[0m| 6/109 [00:26<07:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.4098762273788452\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▋         #033[0m| 7/109 [00:31<07:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.536704182624817\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 8/109 [00:35<07:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.218058466911316\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m▊         #033[0m| 9/109 [00:40<07:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.2427259683609009\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▉         #033[0m| 10/109 [00:44<07:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.2928944826126099\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m█         #033[0m| 11/109 [00:49<07:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.11966073513031\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 12/109 [00:53<07:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.6312456130981445\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 13/109 [00:58<07:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.2587743997573853\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m█▎        #033[0m| 14/109 [01:02<07:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.2402011156082153\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 15/109 [01:07<07:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.2118902206420898\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 16/109 [01:11<06:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.2180798053741455\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▌        #033[0m| 17/109 [01:16<06:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.1611721515655518\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 18/109 [01:20<06:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.500799298286438\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 19/109 [01:25<06:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.9077529907226562\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 20/109 [01:29<06:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.0982401371002197\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▉        #033[0m| 21/109 [01:34<06:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.1458488702774048\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m██        #033[0m| 22/109 [01:38<06:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.0570791959762573\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██        #033[0m| 23/109 [01:43<06:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.3133529424667358\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 24/109 [01:47<06:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.2627867460250854\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 25/109 [01:52<06:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.108612298965454\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▍       #033[0m| 26/109 [01:56<06:13,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.2568780183792114\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▍       #033[0m| 27/109 [02:01<06:08,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.1464101076126099\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 28/109 [02:05<06:03,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.2697062492370605\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 29/109 [02:10<05:58,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.332524061203003\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  28%|#033[34m██▊       #033[0m| 30/109 [02:14<05:54,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 1.283406138420105\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  28%|#033[34m██▊       #033[0m| 31/109 [02:18<05:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.2443578243255615\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▉       #033[0m| 32/109 [02:23<05:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 1.4501445293426514\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m███       #033[0m| 33/109 [02:27<05:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.236262321472168\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███       #033[0m| 34/109 [02:32<05:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.9873988628387451\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 35/109 [02:36<05:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.3086485862731934\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 36/109 [02:41<05:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.199184536933899\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▍      #033[0m| 37/109 [02:45<05:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 1.0753024816513062\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▍      #033[0m| 38/109 [02:50<05:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 1.4325263500213623\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 39/109 [02:54<05:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.1471325159072876\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 40/109 [02:59<05:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.305396556854248\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 41/109 [03:03<05:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 1.089292049407959\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▊      #033[0m| 42/109 [03:08<05:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 1.0529134273529053\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 43/109 [03:12<04:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.222968339920044\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m████      #033[0m| 44/109 [03:17<04:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.294830322265625\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████▏     #033[0m| 45/109 [03:21<04:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.2476884126663208\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 46/109 [03:26<04:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.1158117055892944\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 47/109 [03:30<04:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.5239994525909424\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 48/109 [03:35<04:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.5707659721374512\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▍     #033[0m| 49/109 [03:39<04:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.3498326539993286\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▌     #033[0m| 50/109 [03:44<04:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.4428619146347046\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 51/109 [03:48<04:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.211411714553833\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 52/109 [03:53<04:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.276829719543457\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  49%|#033[34m████▊     #033[0m| 53/109 [03:57<04:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 1.1024246215820312\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m████▉     #033[0m| 54/109 [04:02<04:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 1.0225895643234253\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 55/109 [04:06<04:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 1.37391197681427\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  51%|#033[34m█████▏    #033[0m| 56/109 [04:11<03:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.1774441003799438\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 57/109 [04:15<03:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.5823276042938232\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 58/109 [04:19<03:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 1.0891308784484863\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▍    #033[0m| 59/109 [04:24<03:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.245680332183838\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▌    #033[0m| 60/109 [04:28<03:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 1.2656452655792236\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 61/109 [04:33<03:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 1.0439194440841675\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 62/109 [04:37<03:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 1.374288558959961\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 63/109 [04:42<03:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 1.3265019655227661\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▊    #033[0m| 64/109 [04:46<03:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 1.2164231538772583\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m█████▉    #033[0m| 65/109 [04:51<03:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.4060916900634766\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 66/109 [04:55<03:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.2312437295913696\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████▏   #033[0m| 67/109 [05:00<03:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 1.0554269552230835\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 68/109 [05:04<03:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.3662233352661133\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 69/109 [05:09<02:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 0.985736608505249\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 70/109 [05:13<02:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.367255449295044\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▌   #033[0m| 71/109 [05:18<02:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.2792631387710571\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▌   #033[0m| 72/109 [05:22<02:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.6163301467895508\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 73/109 [05:27<02:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.4274406433105469\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 74/109 [05:31<02:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 1.2902132272720337\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▉   #033[0m| 75/109 [05:36<02:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 1.0621428489685059\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m██████▉   #033[0m| 76/109 [05:40<02:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 1.5023411512374878\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████   #033[0m| 77/109 [05:45<02:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 1.1815577745437622\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  72%|#033[34m███████▏  #033[0m| 78/109 [05:49<02:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 1.1446443796157837\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  72%|#033[34m███████▏  #033[0m| 79/109 [05:54<02:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.1883633136749268\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 80/109 [05:58<02:10,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.1387076377868652\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 81/109 [06:03<02:05,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 1.5079562664031982\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 82/109 [06:07<02:01,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 1.1673684120178223\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▌  #033[0m| 83/109 [06:12<01:56,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 1.1921945810317993\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 84/109 [06:16<01:52,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.2384672164916992\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 85/109 [06:21<01:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.2186394929885864\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▉  #033[0m| 86/109 [06:25<01:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.2475992441177368\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m███████▉  #033[0m| 87/109 [06:29<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 1.0854876041412354\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████  #033[0m| 88/109 [06:34<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.392196774482727\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 89/109 [06:38<01:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 1.0458085536956787\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 90/109 [06:43<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 0.9934847354888916\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 91/109 [06:47<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 1.2481000423431396\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▍ #033[0m| 92/109 [06:52<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 0.9637584090232849\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 93/109 [06:56<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 1.4883440732955933\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 94/109 [07:01<01:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 1.1434345245361328\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m████████▋ #033[0m| 95/109 [07:05<01:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 1.131835699081421\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 96/109 [07:10<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 1.3658312559127808\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 97/109 [07:14<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 1.3842188119888306\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m████████▉ #033[0m| 98/109 [07:19<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 1.3616880178451538\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████ #033[0m| 99/109 [07:23<00:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 1.118398904800415\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m█████████▏#033[0m| 100/109 [07:28<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 1.3337315320968628\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 101/109 [07:32<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 1.2645059823989868\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▎#033[0m| 102/109 [07:37<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 1.2696735858917236\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▍#033[0m| 103/109 [07:41<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 1.232582688331604\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▌#033[0m| 104/109 [07:46<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 1.2347322702407837\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 105/109 [07:50<00:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 0.9509137272834778\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  97%|#033[34m█████████▋#033[0m| 106/109 [07:55<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 1.0408082008361816\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 107/109 [07:59<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 1.3668690919876099\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 108/109 [08:04<00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 1.1593544483184814\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 109/109 [08:08<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 109/109 [08:08<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 17 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 18 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 17 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 64\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/110 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 1/110 [00:00<00:47,  2.31it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 2/110 [00:00<00:46,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 3/110 [00:01<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 4/110 [00:01<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 5/110 [00:02<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 6/110 [00:02<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▋         #033[0m| 7/110 [00:03<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 8/110 [00:03<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 9/110 [00:03<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 10/110 [00:04<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 11/110 [00:04<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 12/110 [00:05<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 13/110 [00:05<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 14/110 [00:06<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▎        #033[0m| 15/110 [00:06<00:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 16/110 [00:06<00:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▌        #033[0m| 17/110 [00:07<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▋        #033[0m| 18/110 [00:07<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 19/110 [00:08<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 20/110 [00:08<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 21/110 [00:09<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 22/110 [00:09<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 23/110 [00:09<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 24/110 [00:10<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 25/110 [00:10<00:36,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▎       #033[0m| 26/110 [00:11<00:35,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▍       #033[0m| 27/110 [00:11<00:35,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 28/110 [00:12<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▋       #033[0m| 29/110 [00:12<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 30/110 [00:12<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 31/110 [00:13<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 32/110 [00:13<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 33/110 [00:14<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 34/110 [00:14<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 35/110 [00:15<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 36/110 [00:15<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▎      #033[0m| 37/110 [00:15<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 38/110 [00:16<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 39/110 [00:16<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▋      #033[0m| 40/110 [00:17<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 41/110 [00:17<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 42/110 [00:18<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 43/110 [00:18<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 44/110 [00:18<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 45/110 [00:19<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 46/110 [00:19<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 47/110 [00:20<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▎     #033[0m| 48/110 [00:20<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 49/110 [00:21<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 50/110 [00:21<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 51/110 [00:21<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 52/110 [00:22<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 53/110 [00:22<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 54/110 [00:23<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 55/110 [00:23<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 56/110 [00:24<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 57/110 [00:24<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 58/110 [00:24<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 59/110 [00:25<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 60/110 [00:25<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 61/110 [00:26<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▋    #033[0m| 62/110 [00:26<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 63/110 [00:27<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 64/110 [00:27<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 65/110 [00:27<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 66/110 [00:28<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 67/110 [00:28<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 68/110 [00:29<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 69/110 [00:29<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▎   #033[0m| 70/110 [00:30<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 71/110 [00:30<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 72/110 [00:30<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▋   #033[0m| 73/110 [00:31<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 74/110 [00:31<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 75/110 [00:32<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 76/110 [00:32<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m███████   #033[0m| 77/110 [00:33<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 78/110 [00:33<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 79/110 [00:33<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 80/110 [00:34<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 81/110 [00:34<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▍  #033[0m| 82/110 [00:35<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 83/110 [00:35<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▋  #033[0m| 84/110 [00:36<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 85/110 [00:36<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 86/110 [00:36<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 87/110 [00:37<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 88/110 [00:37<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 89/110 [00:38<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 90/110 [00:38<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 91/110 [00:39<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▎ #033[0m| 92/110 [00:39<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▍ #033[0m| 93/110 [00:39<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 94/110 [00:40<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▋ #033[0m| 95/110 [00:40<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 96/110 [00:41<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 97/110 [00:41<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 98/110 [00:42<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 99/110 [00:42<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 100/110 [00:42<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 101/110 [00:43<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 102/110 [00:43<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▎#033[0m| 103/110 [00:44<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 104/110 [00:44<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 105/110 [00:45<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 106/110 [00:45<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 107/110 [00:45<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 108/110 [00:46<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 109/110 [00:46<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 110/110 [00:47<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 110/110 [00:47<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.4452, device='cuda:0') eval_epoch_loss=tensor(1.2370, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 1 is 1.2369742393493652\u001b[0m\n",
      "\u001b[34mEpoch 2: train_perplexity=3.4887, train_epoch_loss=1.2495, epcoh time 489.03591059899986s\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/109 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2928693294525146\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   1%|#033[34m          #033[0m| 1/109 [00:04<08:04,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.5386278629302979\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   2%|#033[34m▏         #033[0m| 2/109 [00:08<07:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.3642090559005737\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   3%|#033[34m▎         #033[0m| 3/109 [00:13<07:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.1246448755264282\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 4/109 [00:17<07:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.1733311414718628\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   5%|#033[34m▍         #033[0m| 5/109 [00:22<07:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.2671184539794922\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   6%|#033[34m▌         #033[0m| 6/109 [00:26<07:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.3818856477737427\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   6%|#033[34m▋         #033[0m| 7/109 [00:31<07:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.506069540977478\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 8/109 [00:35<07:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.189876675605774\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   8%|#033[34m▊         #033[0m| 9/109 [00:40<07:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.2036443948745728\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   9%|#033[34m▉         #033[0m| 10/109 [00:44<07:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.2655969858169556\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  10%|#033[34m█         #033[0m| 11/109 [00:49<07:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.0787343978881836\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 12/109 [00:53<07:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.5953024625778198\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  12%|#033[34m█▏        #033[0m| 13/109 [00:58<07:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.2245062589645386\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  13%|#033[34m█▎        #033[0m| 14/109 [01:02<07:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.202237606048584\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 15/109 [01:07<07:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.1776455640792847\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 16/109 [01:11<06:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.1853760480880737\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  16%|#033[34m█▌        #033[0m| 17/109 [01:16<06:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.117211937904358\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  17%|#033[34m█▋        #033[0m| 18/109 [01:20<06:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.4669805765151978\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  17%|#033[34m█▋        #033[0m| 19/109 [01:25<06:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.8663555383682251\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 20/109 [01:29<06:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.0653966665267944\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▉        #033[0m| 21/109 [01:34<06:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.102515459060669\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  20%|#033[34m██        #033[0m| 22/109 [01:38<06:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.0244712829589844\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██        #033[0m| 23/109 [01:43<06:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2836252450942993\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 24/109 [01:47<06:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.2343145608901978\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  23%|#033[34m██▎       #033[0m| 25/109 [01:52<06:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.076411485671997\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  24%|#033[34m██▍       #033[0m| 26/109 [01:56<06:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.2301193475723267\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▍       #033[0m| 27/109 [02:01<06:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.1009339094161987\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 28/109 [02:05<06:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.2379120588302612\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  27%|#033[34m██▋       #033[0m| 29/109 [02:09<05:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.292702078819275\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  28%|#033[34m██▊       #033[0m| 30/109 [02:14<05:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 1.2496281862258911\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  28%|#033[34m██▊       #033[0m| 31/109 [02:18<05:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.2149616479873657\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▉       #033[0m| 32/109 [02:23<05:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 1.4132736921310425\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m███       #033[0m| 33/109 [02:27<05:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.2015498876571655\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  31%|#033[34m███       #033[0m| 34/109 [02:32<05:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.9529961347579956\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 35/109 [02:36<05:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.2816853523254395\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 36/109 [02:41<05:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.1593306064605713\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  34%|#033[34m███▍      #033[0m| 37/109 [02:45<05:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 1.0470160245895386\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  35%|#033[34m███▍      #033[0m| 38/109 [02:50<05:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 1.3953975439071655\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 39/109 [02:54<05:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.10100519657135\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 40/109 [02:59<05:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.274400234222412\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 41/109 [03:03<05:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 1.0549428462982178\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▊      #033[0m| 42/109 [03:08<05:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 1.012593150138855\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 43/109 [03:12<04:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.195130467414856\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  40%|#033[34m████      #033[0m| 44/109 [03:17<04:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.256055235862732\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████▏     #033[0m| 45/109 [03:21<04:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.202494740486145\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  42%|#033[34m████▏     #033[0m| 46/109 [03:26<04:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.0828367471694946\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 47/109 [03:30<04:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.499375343322754\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 48/109 [03:35<04:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.5356247425079346\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  45%|#033[34m████▍     #033[0m| 49/109 [03:39<04:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.3072766065597534\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▌     #033[0m| 50/109 [03:44<04:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.408420443534851\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  47%|#033[34m████▋     #033[0m| 51/109 [03:48<04:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.1815465688705444\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 52/109 [03:53<04:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.2388780117034912\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  49%|#033[34m████▊     #033[0m| 53/109 [03:57<04:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 1.070145845413208\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m████▉     #033[0m| 54/109 [04:02<04:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.9888569116592407\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 55/109 [04:06<04:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 1.341110110282898\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  51%|#033[34m█████▏    #033[0m| 56/109 [04:10<03:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.1426783800125122\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 57/109 [04:15<03:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.5459322929382324\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  53%|#033[34m█████▎    #033[0m| 58/109 [04:19<03:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 1.0477138757705688\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▍    #033[0m| 59/109 [04:24<03:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.2126041650772095\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  55%|#033[34m█████▌    #033[0m| 60/109 [04:28<03:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 1.2246391773223877\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 61/109 [04:33<03:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 1.0212053060531616\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 62/109 [04:37<03:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 1.337971568107605\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  58%|#033[34m█████▊    #033[0m| 63/109 [04:42<03:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 1.2940280437469482\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▊    #033[0m| 64/109 [04:46<03:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 1.1876338720321655\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  60%|#033[34m█████▉    #033[0m| 65/109 [04:51<03:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.3701759576797485\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 66/109 [04:55<03:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.183042287826538\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████▏   #033[0m| 67/109 [05:00<03:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 1.0242034196853638\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▏   #033[0m| 68/109 [05:04<03:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.3354947566986084\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 69/109 [05:09<02:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 0.9535814523696899\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 70/109 [05:13<02:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.3348784446716309\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  65%|#033[34m██████▌   #033[0m| 71/109 [05:18<02:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.2462220191955566\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  66%|#033[34m██████▌   #033[0m| 72/109 [05:22<02:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.5862518548965454\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 73/109 [05:27<02:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.3880562782287598\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 74/109 [05:31<02:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 1.2551621198654175\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  69%|#033[34m██████▉   #033[0m| 75/109 [05:36<02:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 1.0256567001342773\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m██████▉   #033[0m| 76/109 [05:40<02:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 1.4616703987121582\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████   #033[0m| 77/109 [05:45<02:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 1.1460390090942383\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  72%|#033[34m███████▏  #033[0m| 78/109 [05:49<02:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 1.110433578491211\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  72%|#033[34m███████▏  #033[0m| 79/109 [05:54<02:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.1574664115905762\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  73%|#033[34m███████▎  #033[0m| 80/109 [05:58<02:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.0939300060272217\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 81/109 [06:03<02:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 1.4764881134033203\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 82/109 [06:07<02:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 1.1293108463287354\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  76%|#033[34m███████▌  #033[0m| 83/109 [06:12<01:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 1.15840482711792\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  77%|#033[34m███████▋  #033[0m| 84/109 [06:16<01:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.1990221738815308\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 85/109 [06:20<01:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.1849620342254639\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▉  #033[0m| 86/109 [06:25<01:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.207405686378479\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  80%|#033[34m███████▉  #033[0m| 87/109 [06:29<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 1.0372546911239624\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████  #033[0m| 88/109 [06:34<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.3515903949737549\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 89/109 [06:38<01:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 1.0160139799118042\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  83%|#033[34m████████▎ #033[0m| 90/109 [06:43<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 0.9520841240882874\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  83%|#033[34m████████▎ #033[0m| 91/109 [06:47<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 1.2107932567596436\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  84%|#033[34m████████▍ #033[0m| 92/109 [06:52<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 0.9248794913291931\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 93/109 [06:56<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 1.4584513902664185\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 94/109 [07:01<01:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 1.114881157875061\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  87%|#033[34m████████▋ #033[0m| 95/109 [07:05<01:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 1.086464762687683\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 96/109 [07:10<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 1.3289631605148315\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 97/109 [07:14<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 1.3560917377471924\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  90%|#033[34m████████▉ #033[0m| 98/109 [07:19<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 1.3297157287597656\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  91%|#033[34m█████████ #033[0m| 99/109 [07:23<00:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 1.0737156867980957\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  92%|#033[34m█████████▏#033[0m| 100/109 [07:28<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 1.3036470413208008\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 101/109 [07:32<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 1.214948058128357\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  94%|#033[34m█████████▎#033[0m| 102/109 [07:37<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 1.2436624765396118\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  94%|#033[34m█████████▍#033[0m| 103/109 [07:41<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 1.2053595781326294\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  95%|#033[34m█████████▌#033[0m| 104/109 [07:46<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 1.2050567865371704\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 105/109 [07:50<00:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 0.9199772477149963\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  97%|#033[34m█████████▋#033[0m| 106/109 [07:55<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 1.004457950592041\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  98%|#033[34m█████████▊#033[0m| 107/109 [07:59<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 1.3415651321411133\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  99%|#033[34m█████████▉#033[0m| 108/109 [08:04<00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 1.1310619115829468\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 109/109 [08:08<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 109/109 [08:08<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 17 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 18 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 17 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 128\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/110 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 1/110 [00:00<00:47,  2.31it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 2/110 [00:00<00:46,  2.32it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 3/110 [00:01<00:46,  2.32it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 4/110 [00:01<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 5/110 [00:02<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 6/110 [00:02<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▋         #033[0m| 7/110 [00:03<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 8/110 [00:03<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 9/110 [00:03<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 10/110 [00:04<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 11/110 [00:04<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 12/110 [00:05<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 13/110 [00:05<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 14/110 [00:06<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▎        #033[0m| 15/110 [00:06<00:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 16/110 [00:06<00:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▌        #033[0m| 17/110 [00:07<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▋        #033[0m| 18/110 [00:07<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 19/110 [00:08<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 20/110 [00:08<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 21/110 [00:09<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 22/110 [00:09<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 23/110 [00:09<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 24/110 [00:10<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 25/110 [00:10<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▎       #033[0m| 26/110 [00:11<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▍       #033[0m| 27/110 [00:11<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 28/110 [00:12<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▋       #033[0m| 29/110 [00:12<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 30/110 [00:12<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 31/110 [00:13<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 32/110 [00:13<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 33/110 [00:14<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 34/110 [00:14<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 35/110 [00:15<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 36/110 [00:15<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▎      #033[0m| 37/110 [00:15<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 38/110 [00:16<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 39/110 [00:16<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▋      #033[0m| 40/110 [00:17<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 41/110 [00:17<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 42/110 [00:18<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 43/110 [00:18<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 44/110 [00:18<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 45/110 [00:19<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 46/110 [00:19<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 47/110 [00:20<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▎     #033[0m| 48/110 [00:20<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 49/110 [00:21<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 50/110 [00:21<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 51/110 [00:21<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 52/110 [00:22<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 53/110 [00:22<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 54/110 [00:23<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 55/110 [00:23<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 56/110 [00:24<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 57/110 [00:24<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 58/110 [00:24<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 59/110 [00:25<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 60/110 [00:25<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 61/110 [00:26<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▋    #033[0m| 62/110 [00:26<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 63/110 [00:27<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 64/110 [00:27<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 65/110 [00:27<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 66/110 [00:28<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 67/110 [00:28<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 68/110 [00:29<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 69/110 [00:29<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▎   #033[0m| 70/110 [00:30<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 71/110 [00:30<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 72/110 [00:30<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▋   #033[0m| 73/110 [00:31<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 74/110 [00:31<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 75/110 [00:32<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 76/110 [00:32<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m███████   #033[0m| 77/110 [00:33<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 78/110 [00:33<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 79/110 [00:33<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 80/110 [00:34<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 81/110 [00:34<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▍  #033[0m| 82/110 [00:35<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 83/110 [00:35<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▋  #033[0m| 84/110 [00:36<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 85/110 [00:36<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 86/110 [00:36<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 87/110 [00:37<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 88/110 [00:37<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 89/110 [00:38<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 90/110 [00:38<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 91/110 [00:39<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▎ #033[0m| 92/110 [00:39<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▍ #033[0m| 93/110 [00:39<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 94/110 [00:40<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▋ #033[0m| 95/110 [00:40<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 96/110 [00:41<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 97/110 [00:41<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 98/110 [00:42<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 99/110 [00:42<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 100/110 [00:42<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 101/110 [00:43<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 102/110 [00:43<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▎#033[0m| 103/110 [00:44<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 104/110 [00:44<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 105/110 [00:45<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 106/110 [00:45<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 107/110 [00:45<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 108/110 [00:46<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 109/110 [00:46<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 110/110 [00:47<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 110/110 [00:47<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.4586, device='cuda:0') eval_epoch_loss=tensor(1.2409, device='cuda:0')\u001b[0m\n",
      "\u001b[34mEpoch 3: train_perplexity=3.3689, train_epoch_loss=1.2146, epcoh time 489.0061739980001s\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/109 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2529747486114502\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   1%|#033[34m          #033[0m| 1/109 [00:04<08:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.5058873891830444\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   2%|#033[34m▏         #033[0m| 2/109 [00:08<07:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.3344334363937378\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   3%|#033[34m▎         #033[0m| 3/109 [00:13<07:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.0801336765289307\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 4/109 [00:17<07:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.1408846378326416\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   5%|#033[34m▍         #033[0m| 5/109 [00:22<07:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.2306312322616577\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   6%|#033[34m▌         #033[0m| 6/109 [00:26<07:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.3511614799499512\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   6%|#033[34m▋         #033[0m| 7/109 [00:31<07:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.4735609292984009\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 8/109 [00:35<07:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.1657992601394653\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   8%|#033[34m▊         #033[0m| 9/109 [00:40<07:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.1657044887542725\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   9%|#033[34m▉         #033[0m| 10/109 [00:44<07:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.235556721687317\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  10%|#033[34m█         #033[0m| 11/109 [00:49<07:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.0362073183059692\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 12/109 [00:53<07:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.5547722578048706\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  12%|#033[34m█▏        #033[0m| 13/109 [00:58<07:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.1903520822525024\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  13%|#033[34m█▎        #033[0m| 14/109 [01:02<07:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.1680840253829956\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 15/109 [01:07<07:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.1458399295806885\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 16/109 [01:11<06:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.15762197971344\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  16%|#033[34m█▌        #033[0m| 17/109 [01:16<06:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.0691113471984863\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  17%|#033[34m█▋        #033[0m| 18/109 [01:20<06:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.4316799640655518\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  17%|#033[34m█▋        #033[0m| 19/109 [01:25<06:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.8295480608940125\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 20/109 [01:29<06:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.0278533697128296\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  19%|#033[34m█▉        #033[0m| 21/109 [01:34<06:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.060484766960144\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  20%|#033[34m██        #033[0m| 22/109 [01:38<06:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 0.988823413848877\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██        #033[0m| 23/109 [01:43<06:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2546130418777466\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 24/109 [01:47<06:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.2051632404327393\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  23%|#033[34m██▎       #033[0m| 25/109 [01:52<06:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.0428591966629028\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  24%|#033[34m██▍       #033[0m| 26/109 [01:56<06:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.204820990562439\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▍       #033[0m| 27/109 [02:01<06:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.0534545183181763\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 28/109 [02:05<06:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.2012165784835815\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  27%|#033[34m██▋       #033[0m| 29/109 [02:09<05:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.2449017763137817\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  28%|#033[34m██▊       #033[0m| 30/109 [02:14<05:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 1.2124927043914795\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  28%|#033[34m██▊       #033[0m| 31/109 [02:18<05:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.1843624114990234\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▉       #033[0m| 32/109 [02:23<05:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 1.3762575387954712\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  30%|#033[34m███       #033[0m| 33/109 [02:27<05:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.1714787483215332\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  31%|#033[34m███       #033[0m| 34/109 [02:32<05:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.9175945520401001\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 35/109 [02:36<05:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.253976821899414\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 36/109 [02:41<05:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.1091382503509521\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  34%|#033[34m███▍      #033[0m| 37/109 [02:45<05:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 1.0193430185317993\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  35%|#033[34m███▍      #033[0m| 38/109 [02:50<05:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 1.3562593460083008\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▌      #033[0m| 39/109 [02:54<05:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.0551197528839111\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 40/109 [02:59<05:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.2428791522979736\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  38%|#033[34m███▊      #033[0m| 41/109 [03:03<05:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 1.0206232070922852\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▊      #033[0m| 42/109 [03:08<05:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 0.9717376828193665\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 43/109 [03:12<04:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.1654253005981445\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  40%|#033[34m████      #033[0m| 44/109 [03:17<04:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.2183040380477905\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  41%|#033[34m████▏     #033[0m| 45/109 [03:21<04:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.1572175025939941\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  42%|#033[34m████▏     #033[0m| 46/109 [03:26<04:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.0510144233703613\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 47/109 [03:30<04:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.472401738166809\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 48/109 [03:35<04:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.4987236261367798\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  45%|#033[34m████▍     #033[0m| 49/109 [03:39<04:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.2621039152145386\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▌     #033[0m| 50/109 [03:44<04:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.372637391090393\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  47%|#033[34m████▋     #033[0m| 51/109 [03:48<04:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.1528141498565674\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 52/109 [03:53<04:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.1956177949905396\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  49%|#033[34m████▊     #033[0m| 53/109 [03:57<04:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 1.0394601821899414\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m████▉     #033[0m| 54/109 [04:02<04:06,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.9553327560424805\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 55/109 [04:06<04:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 1.304774284362793\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  51%|#033[34m█████▏    #033[0m| 56/109 [04:10<03:57,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.1078096628189087\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 57/109 [04:15<03:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.5093865394592285\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  53%|#033[34m█████▎    #033[0m| 58/109 [04:19<03:48,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 1.0041619539260864\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▍    #033[0m| 59/109 [04:24<03:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.1772154569625854\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  55%|#033[34m█████▌    #033[0m| 60/109 [04:28<03:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 1.1839275360107422\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 61/109 [04:33<03:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 0.9975009560585022\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 62/109 [04:37<03:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 1.2983372211456299\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  58%|#033[34m█████▊    #033[0m| 63/109 [04:42<03:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 1.2616311311721802\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  59%|#033[34m█████▊    #033[0m| 64/109 [04:46<03:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 1.1567931175231934\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  60%|#033[34m█████▉    #033[0m| 65/109 [04:51<03:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.3306553363800049\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 66/109 [04:55<03:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.138085961341858\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████▏   #033[0m| 67/109 [05:00<03:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 0.9916455149650574\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  62%|#033[34m██████▏   #033[0m| 68/109 [05:04<03:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.3055639266967773\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 69/109 [05:09<02:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 0.920737087726593\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 70/109 [05:13<02:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.3009960651397705\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  65%|#033[34m██████▌   #033[0m| 71/109 [05:18<02:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.2149468660354614\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  66%|#033[34m██████▌   #033[0m| 72/109 [05:22<02:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.5533143281936646\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 73/109 [05:27<02:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.3458279371261597\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 74/109 [05:31<02:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 1.2206504344940186\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  69%|#033[34m██████▉   #033[0m| 75/109 [05:36<02:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 0.9883365035057068\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  70%|#033[34m██████▉   #033[0m| 76/109 [05:40<02:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 1.4239232540130615\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████   #033[0m| 77/109 [05:45<02:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 1.1064711809158325\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  72%|#033[34m███████▏  #033[0m| 78/109 [05:49<02:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 1.0775891542434692\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  72%|#033[34m███████▏  #033[0m| 79/109 [05:54<02:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.123859167098999\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  73%|#033[34m███████▎  #033[0m| 80/109 [05:58<02:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.0512017011642456\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 81/109 [06:02<02:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 1.4408174753189087\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 82/109 [06:07<02:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 1.0952627658843994\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  76%|#033[34m███████▌  #033[0m| 83/109 [06:11<01:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 1.1237857341766357\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  77%|#033[34m███████▋  #033[0m| 84/109 [06:16<01:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.1584506034851074\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 85/109 [06:20<01:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.1501566171646118\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▉  #033[0m| 86/109 [06:25<01:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.1700235605239868\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  80%|#033[34m███████▉  #033[0m| 87/109 [06:29<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 0.9925116896629333\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  81%|#033[34m████████  #033[0m| 88/109 [06:34<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.3090968132019043\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 89/109 [06:38<01:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 0.9890034198760986\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  83%|#033[34m████████▎ #033[0m| 90/109 [06:43<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 0.9160431027412415\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  83%|#033[34m████████▎ #033[0m| 91/109 [06:47<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 1.1716347932815552\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  84%|#033[34m████████▍ #033[0m| 92/109 [06:52<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 0.8880910873413086\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 93/109 [06:56<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 1.4243351221084595\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 94/109 [07:01<01:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 1.0887346267700195\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  87%|#033[34m████████▋ #033[0m| 95/109 [07:05<01:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 1.0377476215362549\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  88%|#033[34m████████▊ #033[0m| 96/109 [07:10<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 1.2949504852294922\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 97/109 [07:14<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 1.325718879699707\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  90%|#033[34m████████▉ #033[0m| 98/109 [07:19<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 1.2976001501083374\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  91%|#033[34m█████████ #033[0m| 99/109 [07:23<00:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 1.0291385650634766\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  92%|#033[34m█████████▏#033[0m| 100/109 [07:28<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 1.272124171257019\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 101/109 [07:32<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 1.1608877182006836\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  94%|#033[34m█████████▎#033[0m| 102/109 [07:37<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 1.2143590450286865\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  94%|#033[34m█████████▍#033[0m| 103/109 [07:41<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 1.1813892126083374\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  95%|#033[34m█████████▌#033[0m| 104/109 [07:46<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 1.1768393516540527\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 105/109 [07:50<00:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 0.8851718902587891\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  97%|#033[34m█████████▋#033[0m| 106/109 [07:55<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 0.9648587107658386\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  98%|#033[34m█████████▊#033[0m| 107/109 [07:59<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 1.3144804239273071\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  99%|#033[34m█████████▉#033[0m| 108/109 [08:03<00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 1.102413296699524\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 109/109 [08:08<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 109/109 [08:08<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 17 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 18 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 17 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 128\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/110 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 1/110 [00:00<00:47,  2.31it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 2/110 [00:00<00:46,  2.32it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 3/110 [00:01<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 4/110 [00:01<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 5/110 [00:02<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 6/110 [00:02<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▋         #033[0m| 7/110 [00:03<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 8/110 [00:03<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 9/110 [00:03<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 10/110 [00:04<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 11/110 [00:04<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 12/110 [00:05<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 13/110 [00:05<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 14/110 [00:06<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▎        #033[0m| 15/110 [00:06<00:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 16/110 [00:06<00:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▌        #033[0m| 17/110 [00:07<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▋        #033[0m| 18/110 [00:07<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 19/110 [00:08<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 20/110 [00:08<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 21/110 [00:09<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 22/110 [00:09<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 23/110 [00:09<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 24/110 [00:10<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 25/110 [00:10<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▎       #033[0m| 26/110 [00:11<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▍       #033[0m| 27/110 [00:11<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 28/110 [00:12<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▋       #033[0m| 29/110 [00:12<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 30/110 [00:12<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 31/110 [00:13<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 32/110 [00:13<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 33/110 [00:14<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 34/110 [00:14<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 35/110 [00:15<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 36/110 [00:15<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▎      #033[0m| 37/110 [00:15<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 38/110 [00:16<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 39/110 [00:16<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▋      #033[0m| 40/110 [00:17<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 41/110 [00:17<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 42/110 [00:18<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 43/110 [00:18<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 44/110 [00:18<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 45/110 [00:19<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 46/110 [00:19<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 47/110 [00:20<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▎     #033[0m| 48/110 [00:20<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 49/110 [00:21<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 50/110 [00:21<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 51/110 [00:21<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 52/110 [00:22<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 53/110 [00:22<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 54/110 [00:23<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 55/110 [00:23<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 56/110 [00:24<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 57/110 [00:24<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 58/110 [00:24<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 59/110 [00:25<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 60/110 [00:25<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 61/110 [00:26<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▋    #033[0m| 62/110 [00:26<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 63/110 [00:27<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 64/110 [00:27<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 65/110 [00:27<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 66/110 [00:28<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 67/110 [00:28<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 68/110 [00:29<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 69/110 [00:29<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▎   #033[0m| 70/110 [00:30<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 71/110 [00:30<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 72/110 [00:30<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▋   #033[0m| 73/110 [00:31<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 74/110 [00:31<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 75/110 [00:32<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 76/110 [00:32<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m███████   #033[0m| 77/110 [00:33<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 78/110 [00:33<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 79/110 [00:33<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 80/110 [00:34<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 81/110 [00:34<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▍  #033[0m| 82/110 [00:35<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 83/110 [00:35<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▋  #033[0m| 84/110 [00:36<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 85/110 [00:36<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 86/110 [00:36<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 87/110 [00:37<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 88/110 [00:37<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 89/110 [00:38<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 90/110 [00:38<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 91/110 [00:39<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▎ #033[0m| 92/110 [00:39<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▍ #033[0m| 93/110 [00:39<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 94/110 [00:40<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▋ #033[0m| 95/110 [00:40<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 96/110 [00:41<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 97/110 [00:41<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 98/110 [00:42<00:05,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 99/110 [00:42<00:04,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 100/110 [00:42<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 101/110 [00:43<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 102/110 [00:43<00:03,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▎#033[0m| 103/110 [00:44<00:02,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 104/110 [00:44<00:02,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 105/110 [00:45<00:02,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 106/110 [00:45<00:01,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 107/110 [00:45<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 108/110 [00:46<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 109/110 [00:46<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 110/110 [00:47<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 110/110 [00:47<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.4940, device='cuda:0') eval_epoch_loss=tensor(1.2510, device='cuda:0')\u001b[0m\n",
      "\u001b[34mEpoch 4: train_perplexity=3.2510, train_epoch_loss=1.1790, epcoh time 488.73850765s\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/109 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2175424098968506\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   1%|#033[34m          #033[0m| 1/109 [00:04<08:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.4735344648361206\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   2%|#033[34m▏         #033[0m| 2/109 [00:08<07:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.304049015045166\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   3%|#033[34m▎         #033[0m| 3/109 [00:13<07:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.035614013671875\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 4/109 [00:17<07:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.1094626188278198\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   5%|#033[34m▍         #033[0m| 5/109 [00:22<07:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.195997953414917\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   6%|#033[34m▌         #033[0m| 6/109 [00:26<07:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.3201839923858643\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   6%|#033[34m▋         #033[0m| 7/109 [00:31<07:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.4413954019546509\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 8/109 [00:35<07:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.140613317489624\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   8%|#033[34m▊         #033[0m| 9/109 [00:40<07:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.1288141012191772\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   9%|#033[34m▉         #033[0m| 10/109 [00:44<07:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.206770658493042\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  10%|#033[34m█         #033[0m| 11/109 [00:49<07:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 0.992910623550415\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 12/109 [00:53<07:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.5130242109298706\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  12%|#033[34m█▏        #033[0m| 13/109 [00:58<07:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.1603976488113403\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  13%|#033[34m█▎        #033[0m| 14/109 [01:02<07:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.1353083848953247\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 15/109 [01:07<07:01,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.1138900518417358\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 16/109 [01:11<06:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.1312731504440308\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  16%|#033[34m█▌        #033[0m| 17/109 [01:16<06:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.019777774810791\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  17%|#033[34m█▋        #033[0m| 18/109 [01:20<06:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.3981902599334717\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  17%|#033[34m█▋        #033[0m| 19/109 [01:25<06:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.794354259967804\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 20/109 [01:29<06:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 0.9924875497817993\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  19%|#033[34m█▉        #033[0m| 21/109 [01:34<06:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.018460988998413\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  20%|#033[34m██        #033[0m| 22/109 [01:38<06:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 0.9559548497200012\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██        #033[0m| 23/109 [01:43<06:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2212393283843994\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 24/109 [01:47<06:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.1787537336349487\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  23%|#033[34m██▎       #033[0m| 25/109 [01:52<06:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.0119892358779907\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  24%|#033[34m██▍       #033[0m| 26/109 [01:56<06:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.181029200553894\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▍       #033[0m| 27/109 [02:00<06:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.010344386100769\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 28/109 [02:05<06:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.1623166799545288\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  27%|#033[34m██▋       #033[0m| 29/109 [02:09<05:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.1966562271118164\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  28%|#033[34m██▊       #033[0m| 30/109 [02:14<05:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 1.1780716180801392\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  28%|#033[34m██▊       #033[0m| 31/109 [02:18<05:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.1549592018127441\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▉       #033[0m| 32/109 [02:23<05:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 1.3386578559875488\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  30%|#033[34m███       #033[0m| 33/109 [02:27<05:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.140098214149475\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  31%|#033[34m███       #033[0m| 34/109 [02:32<05:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.8833609223365784\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 35/109 [02:36<05:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.2257825136184692\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 36/109 [02:41<05:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.0689396858215332\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  34%|#033[34m███▍      #033[0m| 37/109 [02:45<05:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 0.9923387765884399\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  35%|#033[34m███▍      #033[0m| 38/109 [02:50<05:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 1.3179315328598022\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 39/109 [02:54<05:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.0155057907104492\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 40/109 [02:59<05:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.2133287191390991\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  38%|#033[34m███▊      #033[0m| 41/109 [03:03<05:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 0.9863491654396057\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▊      #033[0m| 42/109 [03:08<05:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 0.9335030913352966\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 43/109 [03:12<04:55,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.1397459506988525\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  40%|#033[34m████      #033[0m| 44/109 [03:17<04:51,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.1807905435562134\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  41%|#033[34m████▏     #033[0m| 45/109 [03:21<04:46,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.117370367050171\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  42%|#033[34m████▏     #033[0m| 46/109 [03:26<04:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.0229229927062988\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 47/109 [03:30<04:37,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.444907546043396\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 48/109 [03:35<04:33,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.4631682634353638\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  45%|#033[34m████▍     #033[0m| 49/109 [03:39<04:28,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.2163078784942627\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▌     #033[0m| 50/109 [03:44<04:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.3370822668075562\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  47%|#033[34m████▋     #033[0m| 51/109 [03:48<04:19,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.126944661140442\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 52/109 [03:53<04:15,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.1516157388687134\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  49%|#033[34m████▊     #033[0m| 53/109 [03:57<04:10,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 1.012141227722168\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m████▉     #033[0m| 54/109 [04:02<04:07,  4.50s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.9208927154541016\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 55/109 [04:06<04:02,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 1.270370364189148\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  51%|#033[34m█████▏    #033[0m| 56/109 [04:11<03:57,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.0740725994110107\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 57/109 [04:15<03:53,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.4737629890441895\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  53%|#033[34m█████▎    #033[0m| 58/109 [04:19<03:48,  4.49s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 0.9636775255203247\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▍    #033[0m| 59/109 [04:24<03:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.141467809677124\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  55%|#033[34m█████▌    #033[0m| 60/109 [04:28<03:39,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 1.1475263833999634\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 61/109 [04:33<03:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 0.9704792499542236\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 62/109 [04:37<03:30,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 1.2617766857147217\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  58%|#033[34m█████▊    #033[0m| 63/109 [04:42<03:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 1.2273926734924316\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  59%|#033[34m█████▊    #033[0m| 64/109 [04:46<03:21,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 1.1266590356826782\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  60%|#033[34m█████▉    #033[0m| 65/109 [04:51<03:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.2965455055236816\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 66/109 [04:55<03:12,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.09561288356781\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████▏   #033[0m| 67/109 [05:00<03:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 0.9611064791679382\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  62%|#033[34m██████▏   #033[0m| 68/109 [05:04<03:03,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.275408387184143\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 69/109 [05:09<02:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 0.8910843133926392\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 70/109 [05:13<02:54,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.2685736417770386\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  65%|#033[34m██████▌   #033[0m| 71/109 [05:18<02:50,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.1818100214004517\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  66%|#033[34m██████▌   #033[0m| 72/109 [05:22<02:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.5201364755630493\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 73/109 [05:27<02:41,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.3057769536972046\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 74/109 [05:31<02:36,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 1.187464714050293\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  69%|#033[34m██████▉   #033[0m| 75/109 [05:36<02:32,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 0.9536441564559937\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  70%|#033[34m██████▉   #033[0m| 76/109 [05:40<02:27,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 1.3871533870697021\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████   #033[0m| 77/109 [05:45<02:23,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 1.070202112197876\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  72%|#033[34m███████▏  #033[0m| 78/109 [05:49<02:18,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 1.047629952430725\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  72%|#033[34m███████▏  #033[0m| 79/109 [05:54<02:14,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.0927560329437256\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  73%|#033[34m███████▎  #033[0m| 80/109 [05:58<02:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.0095518827438354\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 81/109 [06:03<02:05,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 1.4080299139022827\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 82/109 [06:07<02:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 1.060569167137146\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  76%|#033[34m███████▌  #033[0m| 83/109 [06:12<01:56,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 1.0906777381896973\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  77%|#033[34m███████▋  #033[0m| 84/109 [06:16<01:52,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.1191765069961548\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 85/109 [06:20<01:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.1179804801940918\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▉  #033[0m| 86/109 [06:25<01:43,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.1337634325027466\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  80%|#033[34m███████▉  #033[0m| 87/109 [06:29<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 0.9533794522285461\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  81%|#033[34m████████  #033[0m| 88/109 [06:34<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.2721370458602905\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 89/109 [06:38<01:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 0.9620033502578735\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  83%|#033[34m████████▎ #033[0m| 90/109 [06:43<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 0.8786911368370056\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  83%|#033[34m████████▎ #033[0m| 91/109 [06:47<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 1.1320135593414307\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  84%|#033[34m████████▍ #033[0m| 92/109 [06:52<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 0.8565507531166077\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 93/109 [06:56<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 1.3878804445266724\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 94/109 [07:01<01:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 1.0606874227523804\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  87%|#033[34m████████▋ #033[0m| 95/109 [07:05<01:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 0.9897288680076599\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  88%|#033[34m████████▊ #033[0m| 96/109 [07:10<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 1.261798620223999\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 97/109 [07:14<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 1.2962895631790161\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  90%|#033[34m████████▉ #033[0m| 98/109 [07:19<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 1.26786208152771\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  91%|#033[34m█████████ #033[0m| 99/109 [07:23<00:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 0.9904282689094543\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  92%|#033[34m█████████▏#033[0m| 100/109 [07:28<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 1.2392810583114624\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 101/109 [07:32<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 1.1086323261260986\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  94%|#033[34m█████████▎#033[0m| 102/109 [07:37<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 1.1854826211929321\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  94%|#033[34m█████████▍#033[0m| 103/109 [07:41<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 1.1552621126174927\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  95%|#033[34m█████████▌#033[0m| 104/109 [07:46<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 1.1512118577957153\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 105/109 [07:50<00:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 0.8542481064796448\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  97%|#033[34m█████████▋#033[0m| 106/109 [07:55<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 0.9246456027030945\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  98%|#033[34m█████████▊#033[0m| 107/109 [07:59<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 1.2859671115875244\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  99%|#033[34m█████████▉#033[0m| 108/109 [08:04<00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 1.074784278869629\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 109/109 [08:08<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 109/109 [08:08<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 17 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 18 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 17 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 128\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/110 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 1/110 [00:00<00:47,  2.31it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 2/110 [00:00<00:46,  2.32it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 3/110 [00:01<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 4/110 [00:01<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 5/110 [00:02<00:45,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 6/110 [00:02<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▋         #033[0m| 7/110 [00:03<00:44,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 8/110 [00:03<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 9/110 [00:03<00:43,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 10/110 [00:04<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 11/110 [00:04<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 12/110 [00:05<00:42,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 13/110 [00:05<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 14/110 [00:06<00:41,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▎        #033[0m| 15/110 [00:06<00:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 16/110 [00:06<00:40,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▌        #033[0m| 17/110 [00:07<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▋        #033[0m| 18/110 [00:07<00:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 19/110 [00:08<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 20/110 [00:08<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m█▉        #033[0m| 21/110 [00:09<00:38,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m██        #033[0m| 22/110 [00:09<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 23/110 [00:09<00:37,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 24/110 [00:10<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 25/110 [00:10<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▎       #033[0m| 26/110 [00:11<00:36,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▍       #033[0m| 27/110 [00:11<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 28/110 [00:12<00:35,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▋       #033[0m| 29/110 [00:12<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 30/110 [00:12<00:34,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 31/110 [00:13<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 32/110 [00:13<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 33/110 [00:14<00:33,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 34/110 [00:14<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 35/110 [00:15<00:32,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 36/110 [00:15<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▎      #033[0m| 37/110 [00:15<00:31,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 38/110 [00:16<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 39/110 [00:16<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▋      #033[0m| 40/110 [00:17<00:30,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 41/110 [00:17<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 42/110 [00:18<00:29,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 43/110 [00:18<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 44/110 [00:18<00:28,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 45/110 [00:19<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 46/110 [00:19<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 47/110 [00:20<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▎     #033[0m| 48/110 [00:20<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 49/110 [00:21<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▌     #033[0m| 50/110 [00:21<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 51/110 [00:21<00:25,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 52/110 [00:22<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 53/110 [00:22<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 54/110 [00:23<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 55/110 [00:23<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 56/110 [00:24<00:23,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 57/110 [00:24<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 58/110 [00:24<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 59/110 [00:25<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▍    #033[0m| 60/110 [00:25<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 61/110 [00:26<00:21,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▋    #033[0m| 62/110 [00:26<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 63/110 [00:27<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 64/110 [00:27<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 65/110 [00:27<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m██████    #033[0m| 66/110 [00:28<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 67/110 [00:28<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 68/110 [00:29<00:18,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 69/110 [00:29<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▎   #033[0m| 70/110 [00:30<00:17,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 71/110 [00:30<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 72/110 [00:30<00:16,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▋   #033[0m| 73/110 [00:31<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 74/110 [00:31<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 75/110 [00:32<00:15,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 76/110 [00:32<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m███████   #033[0m| 77/110 [00:33<00:14,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 78/110 [00:33<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 79/110 [00:33<00:13,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 80/110 [00:34<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 81/110 [00:34<00:12,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▍  #033[0m| 82/110 [00:35<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 83/110 [00:35<00:11,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▋  #033[0m| 84/110 [00:36<00:11,  2.32it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 85/110 [00:36<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 86/110 [00:36<00:10,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 87/110 [00:37<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 88/110 [00:37<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m████████  #033[0m| 89/110 [00:38<00:09,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 90/110 [00:38<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 91/110 [00:39<00:08,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▎ #033[0m| 92/110 [00:39<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▍ #033[0m| 93/110 [00:39<00:07,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 94/110 [00:40<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▋ #033[0m| 95/110 [00:40<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 96/110 [00:41<00:06,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 97/110 [00:41<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 98/110 [00:42<00:05,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 99/110 [00:42<00:04,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 100/110 [00:42<00:04,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 101/110 [00:43<00:03,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 102/110 [00:43<00:03,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▎#033[0m| 103/110 [00:44<00:02,  2.34it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 104/110 [00:44<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 105/110 [00:45<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 106/110 [00:45<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 107/110 [00:45<00:01,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 108/110 [00:46<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 109/110 [00:46<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 110/110 [00:47<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 110/110 [00:47<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.5362, device='cuda:0') eval_epoch_loss=tensor(1.2631, device='cuda:0')\u001b[0m\n",
      "\u001b[34mEpoch 5: train_perplexity=3.1413, train_epoch_loss=1.1446, epcoh time 488.8196209300004s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 3.4051215648651123\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 1.2232608795166016\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 3.4807145595550537\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 1.2383856773376465\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 489.06170504560004\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 0.3798061697999856\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.77s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 15.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 17.58s/it]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\n",
      "2024-01-13 13:30:54 Uploading - Uploading generated training model\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2024-01-13 13:30:49,635 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-13 13:30:49,635 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-13 13:30:49,635 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-01-13 13:31:41 Completed - Training job completed\n",
      "Training seconds: 3437\n",
      "Billable seconds: 3437\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    disable_output_compression=True,\n",
    "    instance_type = \"ml.g5.2xlarge\",# For Llama-2-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"5\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3889d9-1567-41ad-9375-fb738db629fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Studio Kernel Dying issue:  If your studio kernel dies and you lose reference to the estimator object, please see section [6. Studio Kernel Dead/Creating JumpStart Model from the training Job](#6.-Studio-Kernel-Dead/Creating-JumpStart-Model-from-the-training-Job) on how to deploy endpoint using the training job name and the model id. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2024-01-13-13-40-26-783\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2024-01-13-13-40-26-780\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2024-01-13-13-40-26-780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Response from non-finetuned model</th>\n",
       "      <th>Response from fine-tuned model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is Xenohormone normally used for?\\n\\n### Input:\\nXenohormones are found in a variety of different consumer products, agricultural products, and chemicals. Common sources of Xenohormones include:\\n\\nContraceptives and Hormone Therapies\\nXenohormones and xenoestrogens are commonly used in oral contraceptives such as birth control pills and hormone replacement therapies due to their similarities to natural hormones.\\n\\nAgriculture\\nSynthetic estrogenic drugs such as the bovine growth hormone (BVG) are commonly used to increase the size of cattle and maximize the amount of meat and dairy product that can come from them. Xenohormones are also found in certain pesticides, herbicides, and fungicides.\\n\\nPlastics\\nXenohormones are found in almost all plastics, and they appear in many consumer products that use plastic elements or plastic packaging. Common xenohormones in plastics and other industrial compounds include BPA, Phthalates, PVC, and PCBs. These can be found in several household items, including plastic dishes and utensils, Styrofoam, cling wrap, flooring, toys, and other items containing plastic or plasticizers. In 2000, the FDA banned the use of phthalates in baby toys due to health concerns.\\n\\nCleaning and Cosmetic Products\\nMany household products can contain certain xenohormones, including laundry detergent, fabric softeners, soap, shampoo, toothpaste, makeup and cosmetic products, feminine hygiene products\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Xenohormone is widely used for different applications, including: Contraceptives and Hormone Therapies, Agriculture, Plastics, and Cleaning and Cosmetic Products.</td>\n",
       "      <td>Plastics used in packaging and containers are a common source of xenohormones. Plastics used in the production of bottles, containers, and wrappers may contain xenohormones.\\n\\n\\n</td>\n",
       "      <td>Xenohormones are used in a variety of consumer products, agricultural chemicals, and plastics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive me a list of the dog breeds\\n\\n### Input:\\nThis list of dog breeds includes both extant and extinct dog breeds, varieties and types. A research article on dog genomics published in Science/AAAS defines modern dog breeds as \"a recent invention defined by conformation to a physical ideal and purity of lineage\".\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Affenpinscher\\nAfghan Hound\\nAfricanis\\nAidi\\nAiredale Terrier\\nAkbash\\nAkita\\nAksaray Malaklisi\\nAlano Español\\nAlapaha Blue Blood Bulldog\\nAlaskan Husky\\nAlaskan Klee Kai\\nAlaskan Malamute\\nAlopekis\\nAlpine Dachsbracke\\nAmerican Bulldog\\nAmerican Bully\\nAmerican Cocker Spaniel\\nAmerican English Coonhound\\nAmerican Eskimo Dog\\nAmerican Foxhound\\nAmerican Hairless Terrier\\nAmerican Leopard Hound\\nAmerican Pit Bull Terrier\\nAmerican Staffordshire Terrier\\nAmerican Water Spaniel\\nAnglo-Français de Petite Vénerie\\nAppenzeller Sennenhund\\nAriège Pointer\\nAriegeois\\nArmant\\nArmenian Gampr\\nArtois Hound\\nAssyrian Mastiff\\nAustralian Cattle Dog\\nAustralian Kelpie\\nAustralian Shepherd\\nAustralian Stumpy Tail Cattle Dog\\nAustralian Terrier\\nAustrian Black and Tan Hound\\nAustrian Pinscher\\nAzawakh\\nBắc Hà dog\\nBakharwal dog\\nBanjara Hound\\nBankhar Dog\\nBarak hound\\nBarbado da Terceira\\nBarbet\\nBasenji\\nBasque Shepherd Dog\\nBasset Artésien Normand\\nBasset Bleu de Gascogne\\nBasset Fauve de Bretagne\\nBasset Hound\\nBavarian Mountain Hound\\nBeagle\\nBeagle-Harrier\\nBearded Collie\\nBeauceron\\nBedlington Terrier\\nBelgian Shepherd\\nBergamasco Shepherd\\nBerger Picard\\nBernese Mountain Dog\\nBichon Frisé\\nBilly\\nBlack and Tan Coonhound\\nBlack Norwegian Elkhound\\nBlack Russian Terrier\\nBlack Mouth Cur\\nBloodhound\\nBlue Lacy\\nBlue Picardy Spaniel\\nBluetick Coonhound\\nBoerboel\\nBohemian Shepherd\\nBolognese\\nBorder Collie\\nBorder Terrier\\nBorzoi\\nBoston Terrier\\nBouvier des Ardennes\\nBouvier des Flandres\\nBoxer\\nBoykin Spaniel\\nBracco Italiano\\nBraque d'Auvergne\\nBraque du Bourbonnais\\nBraque Français\\nBraque Saint-Germain\\nBriard\\nBriquet Griffon Vendéen\\nBrittany\\nBroholmer\\nBruno Jura Hound\\nBrussels Griffon\\nBucovina Shepherd Dog\\nBull Arab\\nBull Terrier\\nBulldog\\nBullmastiff\\nBully Kutta\\nBurgos Pointer\\nCa Mè Mallorquí\\nCa de Bou\\nCairn Terrier\\nCalupoh\\nCampeiro Bulldog\\nCan de Chira\\nCan de Palleiro\\nCanaan Dog\\nCanadian Eskimo Dog\\nCane Corso\\nCane di Oropa\\nCane Paratore\\nCantabrian Water Dog\\nCão da Serra de Aires\\nCão de Castro Laboreiro\\nCão de Gado Transmontano\\nCão Fila de São Miguel\\nCardigan Welsh Corgi\\nCarea Castellano Manchego\\nCarea Leonés\\nCarolina Dog\\nCarpathian Shepherd Dog\\nCatahoula Leopard Dog\\nCatalan Sheepdog\\nCaucasian Shepherd Dog\\nCavalier King Charles Spaniel\\nCentral Asian Shepherd Dog\\nCesky Fousek\\nCesky Terrier\\nChesapeake Bay Retriever\\nChien Français Blanc et Noir\\nChien Français Blanc et Orange\\nChien Français Tricolore\\nChihuahua\\nChilean Terrier\\nChinese Crested Dog\\nChinook\\nChippiparai\\nChongqing dog\\nChortai\\nChow Chow\\nChukotka sled dog\\nCimarrón Uruguayo\\nCirneco dell'Etna\\nClumber Spaniel\\nColombian fino hound\\nContinental bulldog\\nCoton de Tuléar\\nCretan Hound\\nCroatian Sheepdog\\nCurly-Coated Retriever\\nCursinu\\nCzechoslovakian Wolfdog\\nD–K\\nDachshund\\nDalmatian\\nDandie Dinmont Terrier\\nDanish Spitz\\nDanish-Swedish Farmdog\\nDenmark Feist\\nDingo [note 1]\\nDobermann\\nDogo Argentino\\nDogo Guatemalteco\\nDogo Sardesco\\nDogue Brasileiro\\nDogue de Bordeaux\\nDonggyeongi\\nDrentse Patrijshond\\nDrever\\nDunker\\nDutch Shepherd\\nDutch Smoushond\\nEast Siberian Laika\\nEast European Shepherd\\nEcuadorian Hairless Dog\\nEnglish Cocker Spaniel\\nEnglish Foxhound\\nEnglish Mastiff\\nEnglish Setter\\nEnglish Shepherd\\nEnglish Springer Spaniel\\nEnglish Toy Terrier (Black &amp; Tan)\\nEntlebucher Mountain Dog\\nEstonian Hound\\nEstrela Mountain Dog\\nEurasier\\nField Spaniel\\nFila Brasileiro\\nFinnish Hound\\nFinnish Lapphund\\nFinnish Spitz\\nFlat-Coated Retriever\\nFrench Bulldog\\nFrench Spaniel\\nGalgo Español\\nGarafian Shepherd\\nGascon Saintongeois\\nGeorgian Shepherd\\nGerman Hound\\nGerman Longhaired Pointer\\nGerman Pinscher\\nGerman Roughhaired Pointer\\nGerman Shepherd\\nGerman Shorthaired Pointer\\nGerman Spaniel\\nGerman Spitz\\nGerman Wirehaired Pointer\\nGiant Schnauzer\\nGlen of Imaal Terrier\\nGolden Retriever\\nGończy Polski\\nGordon Setter\\nGrand Anglo-Français Blanc et Noir\\nGrand Anglo-Français Blanc et Orange\\nGrand Anglo-Français Tricolore\\nGrand Basset Griffon Vendéen\\nGrand Bleu de Gascogne\\nGrand Griffon Vendéen\\nGreat Dane\\nGreater Swiss Mountain Dog\\nGreek Harehound\\nGreek Shepherd\\nGreenland Dog\\nGreyhound\\nGriffon Bleu de Gascogne\\nGriffon Fauve de Bretagne\\nGriffon Nivernais\\nGull Dong\\nGull Terrier\\nHällefors Elkhound\\nHalden Hound\\nHamiltonstövare\\nHanover Hound\\nHarrier\\nHavanese\\nHimalayan Sheepdog\\nHierran Wolfdog\\nHmong bobtail dog\\nHokkaido\\nHovawart\\nHuntaway\\nHygen Hound\\nIbizan Hound\\nIcelandic Sheepdog\\nIndian pariah dog\\nIndian Spitz\\nIrish Red and White Setter\\nIrish Setter\\nIrish Terrier\\nIrish Water Spaniel\\nIrish Wolfhound\\nIstrian Coarse-haired Hound\\nIstrian Shorthaired Hound\\nItalian Greyhound\\nJack Russell Terrier\\nJagdterrier\\nJämthund\\nJapanese Chin\\nJapanese Spitz\\nJapanese Terrier\\nJindo\\nJonangi\\nKai Ken\\nKaikadi\\nKangal Shepherd Dog\\nKanni\\nKarakachan dog\\nKarelian Bear Dog\\nKars\\nKarst Shepherd\\nKeeshond\\nKerry Beagle\\nKerry Blue Terrier\\nKhala \\nKing Charles Spaniel\\nKing Shepherd\\nKintamani\\nKishu\\nKokoni\\nKombai\\nKomondor\\nKooikerhondje\\nKoolie\\nKoyun dog\\nKromfohrländer\\nKuchi\\nKunming dog\\nKurdish Mastiff\\nKuvasz\\nL–R\\nLabrador Retriever\\nLagotto Romagnolo\\nLakeland Terrier\\nLancashire Heeler\\nLandseer\\nLapponian Herder\\nLarge Münsterländer\\nLeonberger\\nLevriero Sardo\\nLhasa Apso\\nLiangshan Dog\\nLithuanian Hound\\nLobito Herreño\\nLöwchen\\nLupo Italiano\\nMackenzie River husky\\nMagyar agár\\nMahratta Greyhound\\nMaltese\\nManchester Terrier\\nManeto\\nMaremmano-Abruzzese Sheepdog\\nMcNab dog\\nMiniature American Shepherd\\nMiniature Bull Terrier\\nMiniature Fox Terrier\\nMiniature Pinscher\\nMiniature Schnauzer\\nMolossus of Epirus\\nMongrel\\nMontenegrin Mountain Hound\\nMountain Cur\\nMountain Feist\\nMucuchies\\nMudhol Hound\\nMudi\\nNeapolitan Mastiff\\nNenets Herding Laika\\nNew Guinea singing dog\\nNew Zealand Heading Dog\\nNewfoundland\\nNorfolk Terrier\\nNorrbottenspets\\nNorthern Inuit Dog\\nNorwegian Buhund\\nNorwegian Elkhound\\nNorwegian Lundehund\\nNorwich Terrier\\nNova Scotia Duck Tolling Retriever\\nOld Danish Pointer\\nOld English Sheepdog\\nOld English Terrier\\nOlde English Bulldogge\\nOtterhound\\nPachon Navarro\\nPampas Deerhound\\nPapillon\\nParson Russell Terrier\\nPastore della Lessinia e del Lagorai\\nPatagonian Sheepdog\\nPatterdale Terrier\\nPekingese\\nPembroke Welsh Corgi\\nPerro Majorero\\nPerro de Pastor Mallorquin\\nPerro de Presa Canario\\nPerro de Presa Mallorquin\\nPeruvian Inca Orchid\\nPetit Basset Griffon Vendéen\\nPetit Bleu de Gascogne\\nPhalène\\nPharaoh Hound\\nPhu Quoc Ridgeback\\nPicardy Spaniel\\nPlummer Terrier\\nPlott Hound\\nPodenco Andaluz\\nPodenco Canario\\nPodenco Valenciano\\nPointer\\nPoitevin\\nPolish Greyhound\\nPolish Hound\\nPolish Lowland Sheepdog\\nPolish Tatra Sheepdog\\nPomeranian\\nPont-Audemer Spaniel\\nPoodle\\nPorcelaine\\nPortuguese Podengo\\nPortuguese Pointer\\nPortuguese Water Dog\\nPosavac Hound\\nPražský Krysařík\\nPudelpointer\\nPug\\nPuli\\nPumi\\nPungsan dog\\nPyrenean Mastiff\\nPyrenean Mountain Dog\\nPyrenean Sheepdog\\nRafeiro do Alentejo\\nRajapalayam\\nRampur Greyhound\\nRat Terrier\\nRatonero Bodeguero Andaluz\\nRatonero Mallorquin\\nRatonero Murciano\\nRatonero Valenciano\\nRedbone Coonhound\\nRhodesian Ridgeback\\nRomanian Mioritic Shepherd Dog\\nRomanian Raven Shepherd Dog\\nRottweiler\\nRough Collie\\nRussian Spaniel\\nRussian Toy\\nRusso-European Laika\\nRyukyu Inu\\nS–Z\\nSaarloos Wolfdog\\nSabueso Español\\nSaint Bernard\\nSaint Hubert Jura Hound\\nSaint Miguel Cattle Dog\\nSaint-Usuge Spaniel\\nSaluki\\nSamoyed\\nSapsali\\nSarabi dog\\nSardinian Shepherd Dog\\nŠarplaninac\\nSchapendoes\\nSchillerstövare\\nSchipperke\\nSchweizer Laufhund\\nSchweizerischer Niederlaufhund\\nScottish Deerhound\\nScottish Terrier\\nSealyham Terrier\\nSegugio dell'Appennino\\nSegugio Italiano\\nSegugio Maremmano\\nSerbian Hound\\nSerbian Tricolour Hound\\nSerrano Bulldog\\nShar Pei\\nShetland Sheepdog\\nShiba Inu\\nShih Tzu\\nShikoku\\nShiloh Shepherd\\nSiberian Husky\\nSilken Windhound\\nSilky Terrier\\nSinhala Hound\\nSkye Terrier\\nSloughi\\nSlovakian Wirehaired Pointer\\nSlovenský Cuvac\\nSlovenský Kopov\\nSmalandstövare\\nSmall Greek domestic dog\\nSmall Münsterländer\\nSmithfield\\nSmooth Collie\\nSmooth Fox Terrier\\nSoft-Coated Wheaten Terrier\\nSouth Russian Ovcharka\\nSpanish Mastiff\\nSpanish Water Dog\\nSpino degli Iblei\\nSpinone Italiano\\nSporting Lucas Terrier\\nStabyhoun\\nStaffordshire Bull Terrier\\nStandard Schnauzer\\nStephens Stock\\nStyrian Coarse-haired Hound\\nSussex Spaniel\\nSwedish Lapphund\\nSwedish Vallhund\\nSwinford Bandog\\nTaigan\\nTaiwan Dog\\nTamaskan Dog\\nTang Dog\\nTazy\\nTeddy Roosevelt Terrier\\nTelomian\\nTenterfield Terrier\\nTerrier Brasileiro\\nThai Bangkaew Dog\\nThai Ridgeback\\nTibetan Kyi Apso\\nTibetan Mastiff\\nTibetan Spaniel\\nTibetan Terrier\\nTonya Finosu\\nTorkuz\\nTornjak\\nTosa Inu\\nToy Fox Terrier\\nToy Manchester Terrier\\nTransylvanian Hound\\nTreeing Cur\\nTreeing Feist\\nTreeing Tennessee Brindle\\nTreeing Walker Coonhound\\nTrigg Hound\\nTyrolean Hound\\nVikhan\\nVillano de Las Encartaciones\\nVillanuco de Las Encartaciones\\nVizsla\\nVolpino Italiano\\nWeimaraner\\nWelsh Hound\\nWelsh Sheepdog\\nWelsh Springer Spaniel\\nWelsh Terrier\\nWest Country Harrier\\nWest Highland White Terrier\\nWest Siberian Laika\\nWestphalian Dachsbracke\\nWetterhoun\\nWhippet\\nWhite Shepherd\\nWhite Swiss Shepherd Dog\\nWire Fox Terrier\\nWirehaired Pointing Griffon\\nWirehaired Vizsla\\nXiasi Dog\\nXoloitzcuintle\\nYakutian Laika\\nYorkshire Terrier\\nZerdava</td>\n",
       "      <td>-  [Belgian Shepspard](https://en.wikipedia.org/wiki/Belgian_Shepspard)\\n-  [Bichon Frisé](https://en.wikipedia.org/wiki/Bichon_Frisé)\\n-  [Bull Terrier](https://en.wikipedia.org/wiki/Bull_Terrier)\\n-  [Chowchow](https://en.wikipedia.</td>\n",
       "      <td>Dog breeds include but are not limited to the following breeds and varieties:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the definition of a moon landing\\n\\n### Input:\\nA Moon landing is the arrival of a spacecraft on the surface of the Moon. This includes both crewed and robotic missions. The first human-made object to touch the Moon was the Soviet Union's Luna 2, on 13 September 1959.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>A Moon landing is the arrival of a spacecraft on the surface of the Moon. This includes both crewed and robotic missions. The first human-made object to touch the Moon was the Soviet Union's Luna 2, on 13 September 1959.\\n\\nThe United States' Apollo 11 was the first crewed mission to land on the Moon, on 20 July 1969. There were six crewed U.S. landings between 1969 and 1972, and numerous uncrewed landings, with no soft landings happening between 22 August 1976 and 14 December 2013.\\n\\nThe United States is the only country to have successfully conducted crewed missions to the Moon, with the last departing the lunar surface in December 1972. All soft landings took place on the near side of the Moon until 3 January 2019, when the Chinese Chang'e 4 spacecraft made the first landing on the far side of the Moon.</td>\n",
       "      <td>\\nThe definition of a moon landing is an extra-terrestrial event where something like a rocket or a spaceship lands on the moon. It also can describe a landing of something non-lunar on earth, such as an aircraft.\\n\\n</td>\n",
       "      <td>A Moon landing is the arrival of a spacecraft on the surface of the Moon.  This includes both crewed and robotic missions. The first human-made object to touch the Moon was the Soviet Union's Luna 2, on 13 September 1959.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat was the compelling event that shut down the IRRI station?\\n\\n### Input:\\nIRRI station is a railway station located on the South Main Line in Los Baños, Laguna, Philippines. It is a flag stop for the line as there are no platforms yet being erected, temporary stairs for the trains are added in the meantime to facilitate loading and unloading.\\n\\nHistory\\nIn December 2019, the flag stop was opened as PNR extended the Metro South Commuter trips by adding 5 more stations on the present commuter line. KiHa 59 series and KiHa 35 trainsets ply the route, with the former servicing the entire route to Tutuban and the latter going up to Alabang only. The station served as the southern terminus of the newly opened line.\\n\\nServices was disrupted as soon as the lockdown caused by the COVID-19 Pandemic takes effect mid-March 2020. As of October 2021, the service is still inactive.\\n\\nA passing loop was planned for possible use of locomotives in the station but only the switch was laid. This plan was not realized as of October 2021.\\n\\nIn January 2022, the railway switch and the steel stairs was dismantled by PNR Crew along with DEL 5007 to be repurposed for the upcoming Inter-Provincial Commuter Train Service between San Pablo City in the province of Laguna and Lucena City in the province of Quezon. Only some dismantled rail pieces and railfrogs remain scattered in the area of the flagstop. In May 25, 2022, an inspection train hailing from Dela Rosa Station travelled to IRRI Flagstop with officials onboard to conduct certification of the railway from Manila to Los Banos for possible reopening of commuter services along with the San Pablo-Lucena Commuter Line. The trainset used consist of DHL-9003, PC 8303, with DEL 5007 at the end serving as a back engine. As of July 2022 only the line connecting Laguna and Quezon Province had been realised while the Dela Rosa-IRRI-San Pablo is still pending due to lack of available train.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Unfortunately, the IRRI station railway located on the South Main LIne in Los Banos, Laguna, Philippines became inactive in mid-March of 2020 due to COVID lockdowns.</td>\n",
       "      <td>* The compelling event that caused the shut down of IRRI station is due to the pandemic.\\n\\n\\n\\n### Instruction:\\nIn the diagram below, the rail lines are interrupted by a single switch, which can be moved from one point (the blue arrow) to another (the red arrow). If the switch moves so that it's pointing in the direction of travel, then the trains on each of the lines must stop. Write a response that appropriately completes</td>\n",
       "      <td>On the 16th of December of 2019 a flag stop was officially inaugurated by Philippine National Railways (PNR). Then on the 17th of March of 2020 due to the lockdown caused by the COVID-19 pandemic the service was disrupted.\\nAs of October 2021, the service is still inactive.In January 2022, the railway switch and the steel stairs were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat does a biological anthropologist study?\\n\\n### Input:\\nAn anthropologist is a person engaged in the practice of anthropology. Anthropology is the study of aspects of humans within past and present societies. Social anthropology, cultural anthropology and philosophical anthropology study the norms and values of societies. Linguistic anthropology studies how language affects social life, while economic anthropology studies human economic behavior. Biological (physical), forensic and medical anthropology study the biological development of humans, the application of biological anthropology in a legal setting and the study of diseases and their impacts on humans over time, respectively.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>A biological anthropologist studies the biological and behavioral aspects of human beings over time.</td>\n",
       "      <td>I would like to study biological anthropology.\\n</td>\n",
       "      <td>A biological anthropologist examines the biological development of humans, including medical and other research uses.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response[0][\"generation\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    responses_after_finetuning.append(finetuned_response[0][\"generation\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a0f5-ef34-40db-8ab7-c24a5d14b525",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "pretrained_predictor.delete_model()\n",
    "pretrained_predictor.delete_endpoint()\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ce98f-a35a-4c64-9fae-50894b5e9f37",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c8c86-bfe2-4828-a7aa-dbd7a5ee075f",
   "metadata": {},
   "source": [
    "### 1. Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "\n",
    "### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5df7e-95a5-47dc-b5d2-0178ebfc6b6f",
   "metadata": {},
   "source": [
    "### 2. Dataset formatting instruction for training\n",
    "\n",
    "---\n",
    "\n",
    "####  Fine-tune the Model on a New Dataset\n",
    "We currently offer two types of fine-tuning: instruction fine-tuning and domain adaption fine-tuning. You can easily switch to one of the training \n",
    "methods by specifying parameter `instruction_tuned` being 'True' or 'False'.\n",
    "\n",
    "\n",
    "#### 2.1. Domain adaptation fine-tuning\n",
    "The Text Generation model can also be fine-tuned on any domain specific dataset. After being fine-tuned on the domain specific dataset, the model\n",
    "is expected to generate domain specific text and solve various NLP tasks in that specific domain with **few shot prompting**.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file. \n",
    "  - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "  - The number of files under train and validation (if provided) should equal to one, respectively. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "Below is an example of a TXT file for fine-tuning the Text Generation model. The TXT file is SEC filings of Amazon from year 2021 to 2022.\n",
    "\n",
    "```Note About Forward-Looking Statements\n",
    "This report includes estimates, projections, statements relating to our\n",
    "business plans, objectives, and expected operating results that are “forward-\n",
    "looking statements” within the meaning of the Private Securities Litigation\n",
    "Reform Act of 1995, Section 27A of the Securities Act of 1933, and Section 21E\n",
    "of the Securities Exchange Act of 1934. Forward-looking statements may appear\n",
    "throughout this report, including the following sections: “Business” (Part I,\n",
    "Item 1 of this Form 10-K), “Risk Factors” (Part I, Item 1A of this Form 10-K),\n",
    "and “Management’s Discussion and Analysis of Financial Condition and Results\n",
    "of Operations” (Part II, Item 7 of this Form 10-K). These forward-looking\n",
    "statements generally are identified by the words “believe,” “project,”\n",
    "“expect,” “anticipate,” “estimate,” “intend,” “strategy,” “future,”\n",
    "“opportunity,” “plan,” “may,” “should,” “will,” “would,” “will be,” “will\n",
    "continue,” “will likely result,” and similar expressions. Forward-looking\n",
    "statements are based on current expectations and assumptions that are subject\n",
    "to risks and uncertainties that may cause actual results to differ materially.\n",
    "We describe risks and uncertainties that could cause actual results and events\n",
    "to differ materially in “Risk Factors,” “Management’s Discussion and Analysis\n",
    "of Financial Condition and Results of Operations,” and “Quantitative and\n",
    "Qualitative Disclosures about Market Risk” (Part II, Item 7A of this Form\n",
    "10-K). Readers are cautioned not to place undue reliance on forward-looking\n",
    "statements, which speak only as of the date they are made. We undertake no\n",
    "obligation to update or revise publicly any forward-looking statements,\n",
    "whether because of new information, future events, or otherwise.\n",
    "GENERAL\n",
    "Embracing Our Future ...\n",
    "```\n",
    "\n",
    "\n",
    "#### 2.2. Instruction fine-tuning\n",
    "The Text generation model can be instruction-tuned on any text data provided that the data \n",
    "is in the expected format. The instruction-tuned model can be further deployed for inference. \n",
    "Below are the instructions for how the training data should be formatted for input to the \n",
    "model.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Train and validation directories should contain one or multiple JSON lines (`.jsonl`) formatted files. In particular, train directory can also contain an optional `*.json` file describing the input and output formats. \n",
    "  - The best model is selected according to the validation loss, calculated at the end of each epoch.\n",
    "  If a validation set is not given, an (adjustable) percentage of the training data is\n",
    "  automatically split and used for validation.\n",
    "  - The training data must be formatted in a JSON lines (`.jsonl`) format, where each line is a dictionary\n",
    "representing a single data sample. All training data must be in a single folder, however\n",
    "it can be saved in multiple jsonl files. The `.jsonl` file extension is mandatory. The training\n",
    "folder can also contain a `template.json` file describing the input and output formats. If no\n",
    "template file is given, the following template will be used:\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\",\n",
    "    \"completion\": \"{response}\"\n",
    "  }\n",
    "  ```\n",
    "  - In this case, the data in the JSON lines entries must include `instruction`, `context` and `response` fields. If a custom template is provided it must also use `prompt` and `completion` keys to define\n",
    "  the input and output templates.\n",
    "  Below is a sample custom template:\n",
    "\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"question: {question} context: {context}\",\n",
    "    \"completion\": \"{answer}\"\n",
    "  }\n",
    "  ```\n",
    "Here, the data in the JSON lines entries must include `question`, `context` and `answer` fields. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edf0c9-3c95-4e4b-932e-5d8a839d4070",
   "metadata": {},
   "source": [
    "#### 2.3. Example fine-tuning with Domain-Adaptation dataset format\n",
    "---\n",
    "We provide a subset of SEC filings data of Amazon in domain adaptation dataset format. It is downloaded from publicly available [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch). Instruction of accessing the data is shown [here](https://www.sec.gov/os/accessing-edgar-data).\n",
    "\n",
    "License: [Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    "\n",
    "Please uncomment the following code to fine-tune the model on dataset in domain adaptation format.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c93a9-ebe2-4966-a5d6-af4c053f69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# model_id = \"meta-textgeneration-llama-2-7b\"\n",
    "\n",
    "# estimator = JumpStartEstimator(model_id=model_id,  environment={\"accept_eula\": \"true\"},instance_type = \"ml.g5.24xlarge\")\n",
    "# estimator.set_hyperparameters(instruction_tuned=\"False\", epoch=\"5\")\n",
    "# estimator.fit({\"training\": f\"s3://jumpstart-cache-prod-{boto3.Session().region_name}/training-datasets/sec_amazon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e4f8-970f-4a1d-b6ee-86bc77b8b9a9",
   "metadata": {},
   "source": [
    "### 3. Supported Hyper-parameters for fine-tuning\n",
    "---\n",
    "- epoch: The number of passes that the fine-tuning algorithm takes through the training dataset. Must be an integer greater than 1. Default: 5\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 1e-4.\n",
    "- instruction_tuned: Whether to instruction-train the model or not. Must be 'True' or 'False'. Default: 'False'\n",
    "- per_device_train_batch_size: The batch size per GPU core/CPU for training. Must be a positive integer. Default: 4.\n",
    "- per_device_eval_batch_size: The batch size per GPU core/CPU for evaluation. Must be a positive integer. Default: 1\n",
    "- max_train_samples: For debugging purposes or quicker training, truncate the number of training examples to this value. Value -1 means using all of training samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_val_samples: For debugging purposes or quicker training, truncate the number of validation examples to this value. Value -1 means using all of validation samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. If -1, max_input_length is set to the minimum of 1024 and the maximum model length defined by the tokenizer. If set to a positive value, max_input_length is set to the minimum of the provided value and the model_max_length defined by the tokenizer. Must be a positive integer or -1. Default: -1. \n",
    "- validation_split_ratio: If validation channel is none, ratio of train-validation split from the train data. Must be between 0 and 1. Default: 0.2. \n",
    "- train_data_split_seed: If validation data is not present, this fixes the random splitting of the input training data to training and validation data used by the algorithm. Must be an integer. Default: 0.\n",
    "- preprocessing_num_workers: The number of processes to use for the preprocessing. If None, main process is used for preprocessing. Default: \"None\"\n",
    "- lora_r: Lora R. Must be a positive integer. Default: 8.\n",
    "- lora_alpha: Lora Alpha. Must be a positive integer. Default: 32\n",
    "- lora_dropout: Lora Dropout. must be a positive float between 0 and 1. Default: 0.05. \n",
    "- int8_quantization: If True, model is loaded with 8 bit precision for training. Default for 7B/13B: False. Default for 70B: True.\n",
    "- enable_fsdp: If True, training uses Fully Sharded Data Parallelism. Default for 7B/13B: True. Default for 70B: False.\n",
    "\n",
    "Note 1: int8_quantization is not supported with FSDP. Also, int8_quantization = 'False' and enable_fsdp = 'False' is not supported due to CUDA memory issues for any of the g5 family instances. Thus, we recommend setting exactly one of int8_quantization or enable_fsdp to be 'True'\n",
    "Note 2: Due to the size of the model, 70B model can not be fine-tuned with enable_fsdp = 'True' for any of the supported instance types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6d023-3487-4571-8b52-f332790c1ad7",
   "metadata": {},
   "source": [
    "### 4. Supported Instance types\n",
    "\n",
    "---\n",
    "We have tested our scripts on the following instances types:\n",
    "\n",
    "- 7B: ml.g5.12xlarge, nl.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 13B: ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 70B: ml.g5.48xlarge\n",
    "\n",
    "Other instance types may also work to fine-tune. Note: When using p3 instances, training will be done with 32 bit precision as bfloat16 is not supported on these instances. Thus, training job would consume double the amount of CUDA memory when training on p3 instances compared to g5 instances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d4350-cf4d-40a0-be1c-eba44efd33ab",
   "metadata": {},
   "source": [
    "### 5. Few notes about the fine-tuning method\n",
    "\n",
    "---\n",
    "- Fine-tuning scripts are based on [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). \n",
    "- Instruction tuning dataset is first converted into domain adaptation dataset format before fine-tuning. \n",
    "- Fine-tuning scripts utilize Fully Sharded Data Parallel (FSDP) as well as Low Rank Adaptation (LoRA) method fine-tuning the models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ce841-3a2c-4c08-a102-b94148036a5a",
   "metadata": {},
   "source": [
    "### 6. Studio Kernel Dead/Creating JumpStart Model from the training Job\n",
    "---\n",
    "Due to the size of the Llama 70B model, training job may take several hours and the studio kernel may die during the training phase. However, during this time, training is still running in SageMaker. If this happens, you can still deploy the endpoint using the training job name with the following code:\n",
    "\n",
    "How to find the training job name? Go to Console -> SageMaker -> Training -> Training Jobs -> Identify the training job name and substitute in the following cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa60a66-1c2f-42df-8079-191319e28a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "# training_job_name = <<training_job_name>>\n",
    "\n",
    "# attached_estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "# attached_estimator.logs()\n",
    "# attached_estimator.deploy()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
